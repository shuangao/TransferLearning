Training a CNN with millions of parameters on a small dataset could always lead to horrible overfitting. But the idea of supervised pre-training on some huge image datasets could preventing this problem in certain degree. Compare to other initialized strategies according to certain distributions, the pre-trained model is initialized according to the distribution of the specific task. Indeed, this initialization has certain bias as there is no single dataset including all the invariance for natural images\cite{agrawal2014analyzing}, but this bias could be reduced as the pre-trained image dataset increases and the fine-tuning can be benefit from this initialization.
\subsection{Pre-training and Fine-tuning}
We conduct several experiments on both architectures and use different training initialization strategies for both Food-256 and Food-101 datasets. The scratch model is initialized with Gaussian distribution for AlexNet and Xavier algorithm\cite{glorot2010understanding}, which automatically determines the scale of initialization based on the number of input and output neurons. These two initializations are used for training the model for the original ImageNet task. The pre-trained models and fine-tune models are initialized with the weights trained from ImageNet. For the pre-trained models, we just re-train the output layers while all the layers are re-trained for the fine-tune models.
\begin{table}[htbp]
  \centering
  \caption{Top-5 Accuracy in percent on fine-tuned, pre-trained and scratch model for two architectures}
    \begin{tabular}{r|cc|cc}
    \toprule
          & \multicolumn{2}{c|}{AlexNet} & \multicolumn{2}{c}{GoogLeNet} \\    \midrule
     & Food-101   & Food-256   & Food-101   & Food-256 \\
    Fine-tune & \textbf{88.12} & \textbf{85.59} & \textbf{93.51} & \textbf{90.66} \\
    Pre-trained &76.49	&79.26&	82.84	&83.77\\
    Scratch & 78.18 & 75.35 & 90.45 & 81.2 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ft}%
\end{table}%

From Table \ref{tab:ft} we can see that fine-tune from pre-trained model can boost the performance of the CNN for a specific task. It is interesting to see that for the Food-101 task, the accuracy of  the scratch models outperforms the pre-trained models.

Table \ref{tab:cosa} and \ref{tab:cosg} show the weights' cosine similarity of each layer between the fine-tuned models and their pre-trained models. From the results we can see that the weights in the low layer are more similar which implies that these two architectures can learn the hierarchical features as the low level features are similar for most of the tasks and the difference of the objects is determined by the combination of these low level features. From Table \ref{tab:cosa}, we can see that, in AlexNet the weights of the pre-trained and fine-tuned models are extremely similar. This can be caused by two reasons:
\begin{itemize}
  \item Small receptive filed. Since ReLUs are used in Both architectures, vanishing gradients do not exist. Rectified activation function is mathematically given by
      \begin{equation}\label{relu}
        h = \max ({w^T}x,0) = \left\{ {\begin{array}{*{20}{c}}
{{w^T}x}&{{w^T}x > 0}\\
0&{else}
\end{array}} \right.
      \end{equation}

    The ReLU is inactivated when its input is below 0 and its partial derivative is 0. Sparseness can boost the performance of the linear classifier on top, but on the other hand, sparseness will make the more difficult to train especially for fine-tuning. The derivative of the kernel is $\frac{{\partial J}}{{\partial w}} = \frac{{\partial J}}{{\partial y}}\frac{{\partial y}}{{\partial w}} = \frac{{\partial J}}{{\partial y}}*x$ where $\frac{{\partial J}}{{\partial y}}$ denotes the partial derivative of the activation function, $y=w^Tx$ and $x$ denotes the inputs of the layer. The sparseness of the input could lead to sparse kernel derivative. Therefore, the filters of the fine-tuned AlexNet is extremely similar. Compared to large receptive field used in AlexNet, GoogLeNet employs 2 additional $1\times 1$ convolutional layers before the $3\times 3$ and $5\times 5$ convolutional layers. Even though the most critical purpose of the $1\times 1$ convolutional layer is for computational efficiency, these 2 convolutional layers tend to squeeze the sparse input and generate a dense output as the input of the next layer.
  \item The pooling strategy. In AlexNet, max pooling is applied to all the pooling layers between several convolution layers and in back propagation, the max pooling layer will pass the error to the place where it came from. Since it only came from one place of the receptive field, the back propagation error is sparse and it keeps the kernels of the convolution layers stable. In GoogLeNet, even though, there is a max pooling layer in every inception, there are other 3 back propagation errors, from $5\times 5\_reduce, 1\times 1 conv$ and $3\times 3\_reduce$ that can affect the weights of the previous inception.
\end{itemize}
\begin{table}[htbp]
  \centering
  \caption{Cosine similarity of the inceptions between fine-tuned models and scratch model for GoogLeNet}
    \begin{tabular}{r|cccccc}
    \toprule
    \multicolumn{7}{c}{food256} \\
    \midrule
          & \multicolumn{1}{l}{1x1} & \multicolumn{1}{l}{3x3\_reduce} & \multicolumn{1}{l}{3x3} & \multicolumn{1}{l}{5x5\_reduce} & \multicolumn{1}{l}{5x5} & \multicolumn{1}{l}{pool\_proj } \\
    inception\_3a & 0.72  & 0.72  & 0.64  & 0.67  & 0.73  & 0.69 \\
    inception\_3b & 0.59  & 0.64  & 0.53  & 0.70  & 0.60  & 0.56 \\
    inception\_4a & 0.46  & 0.53  & 0.54  & 0.50  & 0.67  & 0.38 \\
    inception\_4b & 0.55  & 0.58  & 0.63  & 0.52  & 0.69  & 0.41 \\
    inception\_4c & 0.63  & 0.64  & 0.63  & 0.57  & 0.68  & 0.52 \\
    inception\_4d & 0.60  & 0.62  & 0.60  & 0.58  & 0.68  & 0.50 \\
    inception\_4e & 0.60  & 0.61  & 0.67  & 0.61  & 0.68  & 0.50 \\
    inception\_5a & 0.51  & 0.53  & 0.58  & 0.48  & 0.60  & 0.39 \\
    inception\_5b & 0.40  & 0.44  & 0.50  & 0.41  & 0.59  & 0.40 \\  \toprule
    \multicolumn{7}{c}{food101} \\ \midrule
          & \multicolumn{1}{l}{1x1 } & \multicolumn{1}{l}{3x3\_reduce} & \multicolumn{1}{l}{3x3} & \multicolumn{1}{l}{5x5\_reduce} & \multicolumn{1}{l}{5x5} & \multicolumn{1}{l}{pool\_proj } \\
    inception\_3a & 0.71  & 0.72  & 0.63  & 0.67  & 0.73  & 0.68 \\
    inception\_3b & 0.56  & 0.63  & 0.50  & 0.71  & 0.60  & 0.53 \\
    inception\_4a & 0.43  & 0.50  & 0.50  & 0.47  & 0.62  & 0.36 \\
    inception\_4b & 0.48  & 0.52  & 0.57  & 0.50  & 0.67  & 0.35 \\
    inception\_4c & 0.57  & 0.61  & 0.59  & 0.53  & 0.63  & 0.47 \\
    inception\_4d & 0.54  & 0.58  & 0.53  & 0.54  & 0.64  & 0.44 \\
    inception\_4e & 0.53  & 0.54  & 0.61  & 0.55  & 0.62  & 0.42 \\
    inception\_5a & 0.43  & 0.47  & 0.53  & 0.45  & 0.57  & 0.34 \\
    inception\_5b & 0.36  & 0.39  & 0.46  & 0.38  & 0.52  & 0.37 \\
    \bottomrule
    \end{tabular}%
  \label{tab:cosg}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Cosine similarity of the layers between fine-tuned models and scratch model for AlexNet}
    \begin{tabular}{r|ccccccc}
    \toprule
          & conv1 & conv2 & conv3 & conv4 & conv5 & fc6   & fc7 \\
    \midrule
    food256 & 0.997 & 0.987 & 0.976 & 0.976 & 0.978 & 0.936 & 0.923 \\
    food101 & 0.996 & 0.984 & 0.963 & 0.960 & 0.963 & 0.925 & 0.933 \\
    \bottomrule
    \end{tabular}%
  \label{tab:cosa}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'google'
\begin{table}[htbp]
  \centering
  \caption{Sparsity}
    \begin{tabular}{r|cccccc}
    \toprule
          & 1x1 & 3x3\_reduce & 3x3  & 5x5\_reduce & 5x5  & pool\_proj  \\
    \midrule
    inception\_3a & 0.516 & 0.515 & 0.658 & 0.408 & 0.611 & 0.632 \\
    inception\_3b & 0.832 & 0.630 & 0.905 & 0.537 & 0.895 & 0.908 \\
    inception\_4a & 0.801 & 0.494 & 0.888 & 0.416 & 0.814 & 0.888 \\
    inception\_4b & 0.518 & 0.483 & 0.661 & 0.350 & 0.701 & 0.749 \\
    inception\_4c & 0.610 & 0.571 & 0.738 & 0.557 & 0.733 & 0.812 \\
    inception\_4d & 0.729 & 0.661 & 0.842 & 0.494 & 0.823 & 0.851 \\
    inception\_4e & 0.852 & 0.503 & 0.859 & 0.324 & 0.923 & 0.886 \\
    inception\_5a & 0.714 & 0.563 & 0.806 & 0.450 & 0.842 & 0.828 \\
    inception\_5b & 0.822 & 0.804 & 0.800 & 0.708 & 0.829 & 0.894 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


%\begin{figure}[h]
%\centering
%\subfigure[GoogLeNet]{
%   \includegraphics[width=0.12\textwidth]{fig/g.png}\label{a}
%}
%\subfigure[GoogLeNet 256]{
%    \includegraphics[width=0.12\textwidth]{fig/g256_un.png}\label{b}
%}
%\subfigure[GoogLeNet 101]{
%    \includegraphics[width=0.12\textwidth]{fig/g101_un.png}\label{c}
%}
%\subfigure[AlexNet]{
%    \includegraphics[width=0.12\textwidth]{fig/a.png}\label{d}
%}
%\subfigure[AlexNet 256]{
%    \includegraphics[width=0.12\textwidth]{fig/a256_un.png}\label{e}
%}
%\subfigure[AlexNet 101]{
%    \includegraphics[width=0.12\textwidth]{fig/a101_un.png}\label{f}
%}
%\caption{}
%\label{fig.errdiff}
%\end{figure}

\subsection{Training across the datasets}
From the previous experiments we can see that pre-training on the ImageNet dataset can boost the performance of the deep convolutional neural network and the knowledge learned from the general recognition problem can be successfully transferred into our specific area. In this part, we will discuss the generalization ability within the our food recognition area.  Zhou et al. trained AlexNet for Scene Recognition across two datasets with identical categories\cite{NIPS2014_Zhou}. But for more complex situation, such as two similar datasets with different categories, we are very interested in exploring whether Deep CNN can still successfully handle. Therefore, we conduct the following experiment to stimulate a more complex real world problem: transferring the knowledge from the fine-tuned Food-101 model on Food-256 dataset and continue fine-tune it on Food-256. To make the experiment more practical, we limited the number of samples per category from Food-256 for training, because in practise, if we want to build a our own model from Deep CNN, the resource is limited and it is exhausted to collect hundreds of labeled images for each category. 

The

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Add caption}
    \begin{tabular}{c|cc|cc}
    \toprule
          & \multicolumn{2}{c|}{AlexNet} & \multicolumn{2}{c}{GoogLeNet} \\
    \midrule
    instances per class & original & 101   & original & 101 \\ \midrule
    20    & 68.8  & \textbf{75.12} & 74.54 & \textbf{77.77} \\
    30    & 73.15 & \textbf{77.02} & 79.21 & \textbf{81.06} \\
    40    & 76.04 & \textbf{80.23} & 81.76 & \textbf{83.52} \\
    50    & 78.9  & \textbf{81.66} & 84.22 & \textbf{85.84} \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%
